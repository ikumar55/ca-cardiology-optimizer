# CALIFORNIA CARDIOLOGY OPTIMIZER - COMPLETE PROJECT PLAN REFERENCE
# Saved for permanent reference and future consultation

# Cardiology Care Optimization System for California
## Project Summary: Spatial Optimization with Historical Validation

### High-Level Overview
This project analyzes optimal cardiology provider distribution in California using graph neural networks and reinforcement learning, then validates the theoretical results against real-world provider movements from 2020-2024. The system demonstrates how machine learning can quantify healthcare access inefficiencies and predict the impact of provider relocations.

### Core Value Proposition
- **Technical**: End-to-end ML pipeline combining geospatial analysis, graph neural networks, and reinforcement learning
- **Validation**: Historical validation using actual physician movements proves model accuracy
- **Insights**: Quantifies the gap between optimal and actual healthcare distribution
- **Impact**: Shows how small provider relocations could dramatically improve patient access

---

## Technology Stack

### Data & Infrastructure
- **DuckDB**: Primary database for all spatial and provider data
- **DVC**: Data version control and pipeline management
- **GeoPandas/Shapely**: Geospatial data processing
- **OSRM**: Travel time calculations between ZIP codes and providers
- **Pandas**: Data manipulation and ETL operations
- **Requests/BeautifulSoup**: Web scraping for historical data

### Machine Learning
- **PyTorch + PyTorch Geometric**: Graph neural network implementation
- **Stable-Baselines3**: Reinforcement learning (PPO algorithm)
- **Scikit-learn**: Classical ML baselines and preprocessing
- **PySAL**: Spatial statistics and autoregressive models
- **Optuna**: Hyperparameter optimization
- **Captum**: Model interpretability and feature attribution

### Visualization & Deployment
- **Streamlit**: Interactive dashboard and web interface
- **Folium/Plotly**: Interactive maps and visualizations
- **Seaborn/Matplotlib**: Statistical plots and charts
- **Docker**: Containerized deployment
- **GitHub Actions**: CI/CD pipeline

---

## Step-by-Step Implementation

### Phase 1: Data Foundation (Weeks 1-2)

#### Step 1.1: Provider Data Collection
**What**: Download and clean California Medical Board physician roster
**Why**: Creates the supply-side foundation - need to know where all cardiologists are located
**Implementation**:
- Scrape CA Medical Board bulk roster CSV from mbc.ca.gov
- Filter for specialty="Cardiology" (~5,200 providers expected)
- Handle data quality issues: deduplicate NPIs, standardize addresses
- Geocode practice addresses to lat/long using Nominatim API (free tier)
- Estimate provider capacity using industry standards (40 patients/week typical)
- Store in DuckDB table: `providers(npi, name, address, lat, lon, capacity_est, practice_type)`
- **Data validation**: Check for reasonable geographic distribution, flag outliers

#### Step 1.2: Demand Signal Construction
**What**: Build ZIP-level cardiac care demand estimates
**Why**: Need demand-side data to identify underserved areas
**Implementation**:
- Pull CDC PLACES ZIP-level coronary heart disease prevalence data
- Download ZIP code population data from US Census
- Multiply CHD prevalence √ó population to get estimated patient counts
- Cross-validate with county-level CA Department of Public Health statistics
- Handle missing data: interpolate using k-nearest neighbors for sparse ZIPs
- Store in DuckDB: `zip_demand(zip_code, population, chd_prevalence, est_patients, lat, lon)`
- **Quality checks**: Ensure prevalence rates are within expected ranges (5-15%)

#### Step 1.3: Travel Time Matrix
**What**: Calculate drive times between every ZIP code and provider
**Why**: Core constraint for access - patients won't travel unlimited distances
**Implementation**:
- Download OpenStreetMap California extract from Geofabrik (~2GB file)
- Set up OSRM (Open Source Routing Machine) Docker container locally
- Extract ZIP code centroids and provider coordinates
- Generate travel time matrix: ~2,600 ZIPs √ó 5,200 providers = 13.5M calculations
- Batch process to avoid memory issues (process in 1000-ZIP chunks)
- Apply 30-minute accessibility threshold (industry standard)
- Cache results as compressed Parquet: `travel_matrix(zip_code, provider_npi, drive_minutes, distance_km)`
- **Performance optimization**: Use spatial indexing to reduce computation time

#### Step 1.4: Historical Movement Data
**What**: Collect physician address changes 2020-2024
**Why**: Critical for validation - need ground truth of actual provider movements
**Implementation**:
- Scrape CA Medical Board monthly roster updates (archive going back to 2020)
- Compare consecutive months to identify address changes
- Parse and standardize address formats for comparison
- Filter for meaningful relocations: >20 miles, inter-county moves only
- Exclude temporary locations, P.O. boxes, administrative offices
- Store in DuckDB: `movements(npi, old_address, new_address, old_county, new_county, move_date, distance_moved, reason_code)`
- **Data cleaning**: Remove retirement/new license entries, focus on practice relocations

#### Step 1.5: Unified Data Pipeline
**What**: Create ETL pipeline combining all data sources
**Why**: Single source of truth for all downstream analysis
**Implementation**:
- Build DVC pipeline with dependency tracking
- Create data quality tests for each source
- Implement incremental updates for monthly refreshes
- Add data versioning and rollback capabilities
- Generate data profiling reports (missing values, distributions, correlations)
- **Output**: Single DuckDB file with all tables, data lineage documentation

### Phase 2: Baseline Analytics (Week 3)

#### Step 2.1: Unmet Demand Index (UDI) Calculation
**What**: Calculate baseline access metric for each ZIP code
**Why**: Establishes KPI‚ÇÄ - the baseline we're trying to improve
**Implementation**:
- For each ZIP: UDI = (estimated_patients) / (accessible_provider_capacity)
- Accessible capacity uses distance decay: capacity / (1 + max(0, travel_time-30)/30)
- This means providers >30 minutes away contribute less to local access
- Label ZIPs: "desert" (UDI > 1.2), "adequate" (0.8-1.2), "overserved" (UDI < 0.8)
- Calculate spatial statistics: Moran's I for clustering, local outliers
- Generate baseline choropleth map using Folium with county boundaries
- **Validation**: Compare against HRSA shortage area designations for sanity check

#### Step 2.2: Exploratory Data Analysis
**What**: Comprehensive statistical analysis of current distribution
**Why**: Understand patterns before building predictive models
**Implementation**:
- Provider density by county: providers per 100k population
- Travel time distributions: median, 90th percentile by ZIP
- Correlation analysis: UDI vs income, age, rural/urban status
- Desert analysis: how many Californians live in cardiac care deserts
- Generate summary statistics table and correlation heatmap
- **Insights target**: "X% of rural Californians travel >45 minutes for cardiology"

#### Step 2.3: Classical ML Baselines
**What**: Implement spatial regression and XGBoost models
**Why**: Need baselines to prove GNN value - "why use deep learning?"
**Implementation**:
- Feature engineering: population density, median income, spatial lag of UDI, distance to nearest urban center
- Spatial lag calculation: average UDI of 5 nearest neighboring ZIPs
- Train Spatial Autoregressive (SAR) model using PySAL library
- Train XGBoost with same features plus interaction terms
- Use rolling cross-validation: train on months 1-6, test on month 7, slide window
- Target variable: change in UDI over 6-month period
- Store baseline predictions, MAE, R¬≤, feature importance
- **Performance target**: Achieve R¬≤ > 0.6 for XGBoost baseline

### Phase 3: Graph Neural Network (Weeks 4-5)

#### Step 3.1: Graph Construction
**What**: Build spatial graph of ZIP codes and providers
**Why**: Captures network effects - shortages in one area affect neighbors
**Implementation**:
- Create bipartite graph: ZIP nodes + provider nodes
- ZIP-ZIP edges: spatial neighbors within 50km (k-nearest neighbors, k=8)
- ZIP-provider edges: all pairs within 60-minute drive time
- Edge weights: inverse distance for ZIP-ZIP, inverse travel time for ZIP-provider
- Node features for ZIPs: [population_65+, median_income, current_UDI, chd_prevalence, rurality_index, spatial_lag_UDI]
- Node features for providers: [capacity_estimate, years_practicing, subspecialty_flags, hospital_affiliation]
- Normalize all features using StandardScaler
- Save as PyTorch Geometric HeteroData object with train/val/test masks
- **Graph statistics**: 2,600 ZIP nodes, 5,200 provider nodes, ~500k edges

#### Step 3.2: GraphSAGE Model Architecture
**What**: Design and implement graph neural network
**Why**: Learns spatial dependencies that classical models miss
**Implementation**:
- Use HeteroGraphSAGE for bipartite graph (different node types)
- Architecture: 2 GraphSAGE layers with 64 hidden units each
- Activation: ReLU for hidden layers, linear for output
- Aggregation: Mean pooling (try max pooling in ablation study)
- Dropout: 0.3 for regularization
- Target: predict change in UDI over next 6 months
- Loss function: MSE with L2 regularization (lambda=0.001)
- **Model size**: ~50k parameters (manageable on single GPU)

#### Step 3.3: Training Pipeline
**What**: Train GNN with proper validation and hyperparameter tuning
**Why**: Ensure model generalizes and isn't overfit
**Implementation**:
- Data splits: 60% train, 20% validation, 20% test (temporal split by date)
- Use Optuna for hyperparameter search: learning rate, hidden units, layers, dropout
- Training: Adam optimizer, learning rate 0.001, batch size 512
- Early stopping: patience=15 epochs, monitor validation MAE
- Learning rate scheduler: reduce on plateau
- Save best model based on validation performance
- Training time: ~2 hours on Google Colab GPU
- **Performance target**: Beat XGBoost baseline by >10% MAE

#### Step 3.4: Model Interpretability & Ablation
**What**: Explain model predictions and validate design choices
**Why**: Proves model isn't a black box - builds trust in results
**Implementation**:
- Use Captum DeepLift for node-level feature importance
- Generate explanations for top-10 predicted "future deserts"
- Ablation study: remove spatial edges, compare performance
- Attention analysis: which neighbors most influence predictions
- Create bar charts showing feature contributions per ZIP
- Save interpretation results as JSON for dashboard
- **Output**: "Model predicts ZIP X will become desert due to 60% weight on neighboring shortage"

### Phase 4: Reinforcement Learning Optimization (Weeks 6-7)

#### Step 4.1: Environment Design
**What**: Create RL environment for provider allocation
**Why**: Finds optimal moves considering trade-offs between access and utilization
**Implementation**:
- State space: [UDI_per_zip (2600), provider_count_per_county (58), county_utilization (58)] = 2716-dim vector
- Action space: discrete, move 1 FTE from county i to county j (58√ó58 = 3364 possible actions)
- Reward function: -0.7 √ó avg_travel_time - 0.3 √ó provider_utilization_variance
- Episode: 20 moves maximum per episode (realistic quarterly reallocation)
- Termination: max moves reached or no improvement for 5 consecutive moves
- Action masking: prevent moves from counties with <2 providers
- State normalization: z-score normalization for stable training
- Use Gymnasium framework with custom environment class

#### Step 4.2: Offline RL with Behavioral Cloning
**What**: Pre-train policy using historical movement data
**Why**: Warm-start RL training, incorporate domain knowledge
**Implementation**:
- Create "expert" dataset from historical movements 2020-2024
- Filter for successful moves (those that improved local UDI)
- Use behavioral cloning: supervised learning to mimic expert actions
- Neural network: 3 hidden layers, 256 units each, ReLU activation
- Train for 50 epochs with cross-entropy loss
- Use this as initialization for RL fine-tuning
- **Dataset size**: ~200 historical moves, augmented with state information

#### Step 4.3: PPO Training
**What**: Train reinforcement learning agent using Proximal Policy Optimization
**Why**: Learns complex allocation strategy better than greedy heuristics
**Implementation**:
- Use Stable-Baselines3 PPO with MlpPolicy
- Network architecture: 2 hidden layers, 512 units each
- Training hyperparameters: learning_rate=3e-4, n_steps=2048, batch_size=64
- PPO specific: clip_range=0.2, ent_coef=0.01, vf_coef=0.5
- Train for 100k steps (takes ~6 hours on Colab GPU)
- Curriculum learning: start with easier scenarios (fewer ZIPs), gradually increase
- Save policy network weights every 10k steps for analysis
- **Performance tracking**: Average reward, episode length, entropy over training

#### Step 4.4: Policy Evaluation & Comparison
**What**: Test trained policy against multiple baselines
**Why**: Quantifies improvement over simple allocation strategies
**Implementation**:
- Baseline 1: Greedy (always move to ZIP with highest UDI)
- Baseline 2: Random allocation
- Baseline 3: Status quo (no moves)
- Baseline 4: Distance-based (move to geographically central locations)
- Evaluation metrics: average travel time, provider utilization, Gini coefficient of access
- Monte Carlo evaluation: run 1000 episodes per policy
- Bootstrap confidence intervals: 95% CI on all metrics
- Statistical significance testing: paired t-tests between policies
- **Target performance**: PPO beats greedy by >20% on combined metric

### Phase 5: Historical Validation (Week 8)

#### Step 5.1: Natural Experiment Analysis
**What**: Validate model predictions using actual provider movements
**Why**: Proves theoretical optimization works in practice - this is the key credibility piece
**Implementation**:
- Identify counties with significant provider changes 2020-2024 (>2 FTE change)
- For each historical move, recreate the pre-move state
- Run trained models (GNN + RL) to predict impact on local UDI
- Calculate actual UDI change using post-move data
- Compute correlation between predicted and observed improvements
- Statistical analysis: R¬≤, MAE, directional accuracy (% correct up/down predictions)
- **Expected sample size**: 20-30 significant county-level changes

#### Step 5.2: Counterfactual Impact Analysis  
**What**: Compare optimal vs actual historical allocation decisions
**Why**: Shows opportunity cost of current distribution patterns
**Implementation**:
- Take 2020 as baseline state
- Run RL policy to generate optimal allocation for 2020-2024 period
- Compare recommended moves to actual moves that occurred
- Calculate access improvements: optimal path vs actual path vs no-change baseline
- Generate "what-if" scenarios: "If top 10 RL recommendations had been followed..."
- Statistical bootstrapping: confidence intervals on counterfactual gains
- **Key insight**: "Optimal allocation would have improved access 40% more than actual changes"

#### Step 5.3: Model Reliability Analysis
**What**: Test model performance across different scenarios and time periods
**Why**: Ensures results are robust, not just lucky on specific data
**Implementation**:
- Temporal validation: train on 2020-2022, test on 2023-2024 data
- Geographic cross-validation: train on Northern CA, test on Southern CA
- Sensitivity analysis: how do results change with different UDI thresholds (1.0, 1.2, 1.5)
- Stress testing: model performance during COVID-19 disruption period
- Error analysis: where does the model fail, what are systematic biases
- **Robustness metrics**: Performance consistency across validation folds

### Phase 6: Dashboard & Visualization (Week 9)

#### Step 6.1: Interactive Map Interface
**What**: Streamlit dashboard with layered geospatial visualizations
**Why**: Makes complex analysis accessible to non-technical stakeholders
**Implementation**:
- Base layer: California county boundaries with provider density heat map
- Choropleth layers: current UDI, predicted future UDI, optimal UDI
- Point layers: individual provider locations (clustered for performance)
- Toggle controls: switch between current/greedy/PPO scenarios
- Click interactions: ZIP code details popup with metrics
- Color scheme: red-yellow-green scale for intuitive UDI interpretation
- Performance optimization: use deck.gl for smooth rendering of large datasets
- **User experience**: Load time <3 seconds, smooth pan/zoom

#### Step 6.2: A/B Testing Dashboard
**What**: Side-by-side comparison of allocation strategies
**Why**: Demonstrates quantitative value of sophisticated ML approach
**Implementation**:
- Split-screen view: current vs optimal allocation
- KPI comparison table: travel time, utilization, desert count, affected population
- Statistical significance indicators: confidence intervals, p-values
- Interactive sliders: "Budget for N provider moves", see real-time impact
- Export functionality: download results as CSV/PDF report
- **Key metrics display**: -15% travel time, +8pp utilization, -25% desert population

#### Step 6.3: Historical Validation Dashboard
**What**: Show model validation using real-world data
**Why**: Builds credibility by proving predictions match reality
**Implementation**:
- Scatter plot: predicted vs actual UDI improvements (with R¬≤ displayed)
- Time series: model predictions vs actual access changes 2020-2024
- Case study cards: specific counties where predictions were validated
- Error analysis: histogram of prediction errors, outlier identification
- Interactive timeline: select date range, see model performance over time
- **Credibility statement**: "Model accurately predicted 78% of access changes"

#### Step 6.4: Policy Insights Interface
**What**: High-level insights and recommendations dashboard
**Why**: Translates technical results into actionable policy insights
**Implementation**:
- Executive summary: key findings in bullet points
- Impact calculator: "Moving N providers would affect X patients"
- Priority ranking: which counties would benefit most from additional providers
- Cost-benefit analysis: estimated cost per patient-hour saved
- Comparison to benchmarks: California vs other states, rural vs urban
- **Policy recommendations**: Specific, actionable suggestions with quantified impact

### Phase 7: Deployment & Documentation (Week 10)

#### Step 7.1: Production-Ready Deployment
**What**: Containerized application with CI/CD pipeline
**Why**: Demonstrates DevOps skills and enables easy sharing/scaling
**Implementation**:
- Multi-stage Dockerfile: OSRM server + Python environment + Streamlit app
- Docker-compose: orchestrate OSRM, database, and web application
- Environment variables: configurable parameters, API keys, data paths
- Health checks: endpoint monitoring, automatic restart on failure
- Resource limits: memory/CPU constraints for stable performance
- GitHub Actions workflow: automated testing, linting, deployment
- **Deployment targets**: Local development, Streamlit Cloud, optional AWS/GCP

#### Step 7.2: Code Quality & Testing
**What**: Comprehensive testing suite and code documentation
**Why**: Professional development practices, code maintainability
**Implementation**:
- Unit tests: pytest for all data processing functions
- Integration tests: end-to-end pipeline validation
- Data validation: great_expectations for data quality checks
- Code coverage: target >80% coverage, generate HTML reports
- Linting: black, flake8, mypy for code quality
- Pre-commit hooks: automated code formatting and testing
- **Quality metrics**: Zero critical issues, documented functions, type hints

#### Step 7.3: Documentation Package
**What**: Comprehensive project documentation for multiple audiences
**Why**: Essential for portfolio presentation, code maintenance, and knowledge transfer
**Implementation**:
- README.md: project overview, setup instructions, usage examples
- Technical documentation: methodology deep-dive, model architecture, validation approach  
- API documentation: function signatures, parameter descriptions
- Jupyter notebooks: exploratory analysis, model training, results visualization
- Video presentation: 5-minute technical walkthrough, 2-minute executive summary
- Blog post: medium-style article explaining the project and insights
- **Portfolio materials**: One-page PDF summary, resume bullets, presentation slides

#### Step 7.4: Performance Monitoring & Maintenance
**What**: Setup for ongoing model monitoring and updates
**Why**: Shows understanding of ML systems lifecycle management
**Implementation**:
- Data drift detection: monitor feature distributions over time
- Model performance tracking: log prediction accuracy on new data
- Automated retraining pipeline: trigger when performance degrades
- Alert system: notifications for data quality issues or model failures
- Usage analytics: track dashboard interactions, popular features
- **Maintenance plan**: Monthly data updates, quarterly model retraining

---

## Key Success Metrics & Validation

### Technical Performance Targets
- **GNN Model**: Beat XGBoost baseline by >10% MAE on UDI prediction
- **RL Policy**: Achieve >15% travel time reduction vs greedy baseline
- **Historical Validation**: R¬≤ > 0.6 between predicted and actual UDI changes
- **System Performance**: Dashboard loads <3 seconds, handles 1000+ concurrent users

### Portfolio Impact Metrics
- **Quantified Results**: "Reduced travel time 15%, improved utilization 8pp"
- **Scale Demonstration**: "Analyzed 5,200 providers across 2,600 ZIP codes"
- **Validation Credibility**: "78% accuracy predicting real-world access changes"
- **Technical Breadth**: "End-to-end ML pipeline: data engineering ‚Üí GNN ‚Üí RL ‚Üí deployment"

### Data Quality Assurance
- **Completeness**: <5% missing data across all sources
- **Accuracy**: Provider locations within 100m of actual addresses
- **Consistency**: Historical data validated against multiple sources
- **Currency**: Data no older than 6 months at project completion

---

## Risk Mitigation & Contingency Plans

### Technical Risks
- **OSRM Setup Complexity**: Fallback to Google Maps API for travel times (budget $50)
- **Historical Data Availability**: If movement data unavailable, use synthetic validation
- **Model Training Time**: Pre-computed features, smaller graph if performance issues
- **Deployment Issues**: Local-only deployment acceptable for portfolio demonstration

### Data Quality Risks  
- **Provider Data Staleness**: Cross-validate with multiple sources, flag uncertain records
- **Geocoding Failures**: Manual verification for critical providers, interpolation for missing
- **Missing Historical Records**: Focus on available time period, acknowledge limitations
- **Travel Time Accuracy**: Validate sample routes manually, add uncertainty bounds

### Scope Management
- **Feature Creep**: Stick to core pipeline, document extensions for future work
- **Perfectionism**: Aim for "good enough" on first iteration, iterate if time allows
- **Over-Engineering**: Simple solutions preferred, complexity only where it adds clear value
- **Timeline Pressure**: Core MVP (data + baseline + simple optimization) must work first




This comprehensive plan provides enough detail for task breakdown while maintaining focus on the core value proposition: a validated, end-to-end ML system that demonstrates both technical sophistication and real-world applicability.

## ORGANIZATIONAL REQUIREMENTS SUMMARY

### TASK-BASED FOLDER STRUCTURE
Each task must create its own dedicated folder:
```
Task1_DataCollection/
‚îú‚îÄ‚îÄ data/           # Raw and intermediate data
‚îú‚îÄ‚îÄ scripts/        # Code for this task
‚îú‚îÄ‚îÄ results/        # Final outputs for next tasks
‚îî‚îÄ‚îÄ documentation/  # Progress logs and issues
    ‚îî‚îÄ‚îÄ Task1_Implementation_diary.txt
```

### TASK & SUBTASK GRANULARITY ‚≠ê CRITICAL
- Tasks: Each task must have ONE specific, measurable goal
- Subtasks: Break each task into as many subtasks as needed - each subtask should be:
  - Completable in one focused work session (30-60 minutes max)
  - Have ONE specific atomic objective
  - Produce specific, measurable outputs
  - If a subtask has multiple steps that could take >60 minutes ‚Üí break it further
- No limit on number: Create as many tasks and subtasks as needed for granular control
- Dependencies: Must be crystal clear with exact file paths and formats

### MANDATORY DOCUMENTATION IN EACH SUBTASK
Every subtask must end with this requirement:
üìù DOCUMENTATION REQUIREMENTS:
- Log detailed progress to TaskX/documentation/TaskX_Implementation_diary.txt
- Document what was accomplished, how it was done, any issues encountered
- Note any route changes or important decisions made
- Update Task Master: mark subtask complete OR add new subtasks if discovered
- Include exact file paths and formats for outputs

### SEAMLESS FLOW DESIGN
Each task AND subtask must specify:
- Inputs: Exact file paths from previous tasks (e.g., "Task2/results/providers.csv")
- Process: 3-4 specific steps
- Outputs: Exact deliverables with naming conventions and schemas
- Handoff: What the next task needs and where to find it
- Validation: How to verify success before proceeding

### BREAKDOWN PHILOSOPHY ‚≠ê KEY
- Better too granular than too broad: Err on the side of more tasks/subtasks
- Each piece should be "obvious": When someone reads a subtask, it should be immediately clear what to do
- No guessing: Every input file, output format, validation step should be explicitly stated
- Perfect handoffs: Subtask N.3 outputs exactly what Subtask N.4 needs as input 